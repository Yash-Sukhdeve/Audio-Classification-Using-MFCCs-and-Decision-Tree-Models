# Audio Classification with MFCCs and Machine Learning

This project demonstrates a machine learning pipeline for classifying audio samples of spoken digits (0-9). It uses Mel-Frequency Cepstral Coefficients (MFCCs) as the primary features and explores the performance of two different classifiers: a Random Forest and a Support Vector Machine (SVM) with Principal Component Analysis (PCA).

The goal of this project is to build a lightweight and accurate model suitable for deployment on resource-constrained devices like microcontrollers.

## Table of Contents

- [Audio Classification with MFCCs and Machine Learning](#audio-classification-with-mfccs-and-machine-learning)
  - [Table of Contents](#table-of-contents)
  - [Project Overview](#project-overview)
  - [Folder Structure](#folder-structure)
  - [File Descriptions](#file-descriptions)
  - [Setup and Installation](#setup-and-installation)
  - [How to Run the Project](#how-to-run-the-project)
  - [Results](#results)
  - [Acknowledgments](#acknowledgments)

## Project Overview

The project follows these main steps:

1.  **Data Preparation:** The `split_files.py` script splits the original dataset into training and testing sets.
2.  **Feature Extraction:** The `Audio_Classification_org.ipynb` notebook extracts features from the audio files. It explores two main approaches:
    *   **MFCCs:** Mel-Frequency Cepstral Coefficients, which are a standard feature for audio processing.
    *   **PCA:** Principal Component Analysis on a larger set of audio features to reduce dimensionality.
3.  **Model Training and Evaluation:** The notebook trains and evaluates two classifiers:
    *   **Random Forest:** Trained on the MFCC features.
    *   **SVM:** Trained on the PCA-reduced features.
4.  **Analysis:** The notebook analyzes the performance of both models by varying the number of features (MFCC coefficients or PCA components) and plots the results.

## Folder Structure

The repository is organized as follows:

```
.
├── free-spoken-digit-dataset-master/  # The original dataset
│   └── recordings/
├── .gitattributes
├── Audio_Classification_org.ipynb     # The main Jupyter Notebook
├── Readme.md                          # This file
├── split_files.py                     # Script to split the data
├── Images/                            # (Generated) Contains output plots
├── Test/                              # (Generated) Test audio files
├── Train/                             # (Generated) Training audio files
├── test_extracted/                    # (Generated) Extracted test features
├── train_extracted/                   # (Generated) Extracted training features
├── test_idx.csv                       # (Generated) Test set information
├── train.csv                          # (Generated) Training set information
├── predict_rf.csv                     # (Generated) Predictions from Random Forest
└── predict_svm.csv                    # (Generated) Predictions from SVM
```

-   **`free-spoken-digit-dataset-master/`**: Contains the original audio files from the Free Spoken Digit Dataset.
-   **Generated Folders and Files**: The other folders and CSV files are generated by the scripts when you run the project.

## File Descriptions

-   **`split_files.py`**: This Python script prepares the dataset for training and testing. It takes the raw audio files from the `free-spoken-digit-dataset-master/recordings` directory and splits them into two new directories: `Train` and `Test`. It also creates two CSV files, `train.csv` and `test_idx.csv`, which contain the filenames and corresponding labels for the training and testing sets.

-   **`Audio_Classification_org.ipynb`**: This is the main Jupyter Notebook where the magic happens. It performs the following tasks:
    -   **Loads the data:** Reads the `train.csv` and `test_idx.csv` files.
    -   **Extracts features:** Defines functions to extract MFCC and other audio features.
    -   **Trains models:** Implements and trains the Random Forest and SVM classifiers.
    -   **Evaluates models:** Uses 5-fold cross-validation to evaluate the models and find the best hyperparameters.
    -   **Visualizes results:** Generates plots of accuracy and F1-score, and saves them in the `Images/` directory.

## Setup and Installation

To run this project, you'll need Python 3 and the following libraries. You can install them using `pip`:

```bash
pip install numpy pandas matplotlib scikit-learn librosa tensorflow keras
```

**Python Version:** This project was tested with Python 3.11.

## How to Run the Project

Follow these steps to run the project from start to finish:

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```

2.  **Run the data splitting script:**
    ```bash
    python3 split_files.py
    ```
    This will create the `Train/` and `Test/` directories, as well as the `train.csv` and `test_idx.csv` files.

3.  **Run the Jupyter Notebook:**
    You can either open and run the `Audio_Classification_org.ipynb` notebook in a Jupyter environment, or you can run the provided `run_notebook_refactored.py` script which contains the same code:
    ```bash
    python3 run_notebook_refactored.py
    ```
    This will perform the feature extraction, model training, and evaluation. The results will be printed to the console, and the plots will be saved in the `Images/` directory.

## Results

The project evaluates two different models:

-   **Random Forest with MFCCs:** The accuracy and F1-score are evaluated for a range of MFCC coefficient numbers.
-   **SVM with PCA:** The accuracy and F1-score are evaluated for a range of PCA components.

The results, including accuracy, F1-score, and confidence intervals, are printed to the console during the execution of the notebook. The final plots comparing the performance of the models are saved in the `Images/` directory.

Based on the results, the SVM classifier with PCA features generally outperforms the Random Forest with MFCCs for this dataset.

## Acknowledgments

This project was conducted with aid from Dr. Imtiaz as well as the help of online resources including ChatGPT. The dataset used for this project was the **Free Spoken Digit Dataset** which can be found here:

https://github.com/Jakobovski/free-spoken-digit-dataset

The signal processing code is a modified form of this project which proved an invaluable resource on conducting audio processing within python:

https://github.com/ksrvap/Audio-classification-using-SVM-and-CNN